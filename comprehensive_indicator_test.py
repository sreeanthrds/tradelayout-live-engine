#!/usr/bin/env python3
"""
Comprehensive Indicator Test Suite
Tests all 141 ta_hybrid indicators with backtest validation
Date: Oct 1, 2024
"""
import os
import sys
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

os.environ['SUPABASE_URL'] = 'https://oonepfqgzpdssfzvokgk.supabase.co'
os.environ['SUPABASE_SERVICE_ROLE_KEY'] = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Im9vbmVwZnFnenBkc3NmenZva2drIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc1MDE5OTkxNCwiZXhwIjoyMDY1Nzc1OTE0fQ.qmUNhAh3oVhPW2lcAkw7E2Z19MenEIoWCBXCR9Hq6Kg'

import ta_hybrid as ta
import pandas as pd
import numpy as np
from datetime import date, datetime, timedelta
import json
import traceback

# Test results storage
test_results = {
    'total_tested': 0,
    'passed': 0,
    'failed': 0,
    'skipped': 0,
    'categories': {},
    'detailed_results': []
}

print(f"\n{'='*100}")
print(f"COMPREHENSIVE TA_HYBRID INDICATOR TEST SUITE")
print(f"Test Date: October 1, 2024")
print(f"Testing {len(ta._INDICATOR_REGISTRY)} indicators")
print(f"{'='*100}\n")

# Generate synthetic OHLCV data for testing
print("üìä Generating synthetic OHLCV data...")
try:
    # Generate 600 candles (enough for most indicators with lookback periods)
    np.random.seed(42)  # For reproducibility
    num_candles = 600
    
    # Simulate realistic price movement
    base_price = 25800
    timestamps = pd.date_range(start='2024-09-27 09:15:00', periods=num_candles, freq='1min')
    
    # Generate realistic OHLCV data
    close_prices = base_price + np.cumsum(np.random.randn(num_candles) * 10)
    
    # Create realistic OHLC based on close
    data = []
    for i, close in enumerate(close_prices):
        high = close + abs(np.random.randn() * 5)
        low = close - abs(np.random.randn() * 5)
        open_price = close_prices[i-1] if i > 0 else close
        volume = int(1000 + abs(np.random.randn() * 500))
        
        data.append({
            'timestamp': timestamps[i],
            'open': max(low, min(high, open_price)),
            'high': high,
            'low': low,
            'close': close,
            'volume': volume
        })
    
    df = pd.DataFrame(data)
    df = df.set_index('timestamp')
    
    print(f"‚úÖ Generated {len(df)} candles")
    print(f"   Date range: {df.index[0]} to {df.index[-1]}")
    print(f"   Price range: {df['close'].min():.2f} to {df['close'].max():.2f}")
    
    # Split into historical (for initialization) and test data
    oct1_start = pd.Timestamp('2024-10-01 09:15:00')
    historical_df = df[df.index < oct1_start].copy()
    test_df = df[df.index >= oct1_start].copy()
    
    print(f"   Historical candles: {len(historical_df)}")
    print(f"   Test candles (Oct 1): {len(test_df)}")
    
except Exception as e:
    print(f"‚ùå Error generating data: {e}")
    traceback.print_exc()
    sys.exit(1)

print(f"\n{'='*100}")
print(f"TESTING INDICATORS")
print(f"{'='*100}\n")

# Test each indicator
for idx, (name, indicator_class) in enumerate(sorted(ta._INDICATOR_REGISTRY.items()), 1):
    test_result = {
        'name': name,
        'class': indicator_class.__name__,
        'status': 'unknown',
        'error': None,
        'calculation_time_ms': 0,
        'output_columns': [],
        'sample_values': {},
        'has_nans': False,
        'value_range': {}
    }
    
    try:
        print(f"{idx:3d}. Testing {name:<25} ", end='', flush=True)
        
        # Create indicator instance with default params
        start_time = datetime.now()
        
        try:
            indicator = indicator_class()
        except TypeError:
            # Try with common default parameters
            try:
                indicator = indicator_class(length=14)
            except TypeError:
                try:
                    indicator = indicator_class(period=14)
                except TypeError:
                    indicator = indicator_class(fast_period=12, slow_period=26, signal_period=9)
        
        # Calculate indicator on historical data using calculate_bulk
        result = indicator.calculate_bulk(historical_df.copy())
        
        calc_time = (datetime.now() - start_time).total_seconds() * 1000
        test_result['calculation_time_ms'] = round(calc_time, 2)
        
        # Analyze results
        if isinstance(result, pd.DataFrame):
            test_result['output_columns'] = list(result.columns)
            
            # Get sample values from last row
            last_values = result.iloc[-1]
            for col in result.columns:
                val = last_values[col]
                test_result['sample_values'][col] = float(val) if pd.notna(val) and not np.isinf(val) else None
                
                # Check for NaNs
                if result[col].isna().any():
                    test_result['has_nans'] = True
                
                # Get value range (excluding NaNs and Infs)
                valid_values = result[col].replace([np.inf, -np.inf], np.nan).dropna()
                if len(valid_values) > 0:
                    test_result['value_range'][col] = {
                        'min': float(valid_values.min()),
                        'max': float(valid_values.max()),
                        'mean': float(valid_values.mean())
                    }
        
        elif isinstance(result, pd.Series):
            test_result['output_columns'] = [result.name or 'value']
            val = result.iloc[-1]
            test_result['sample_values']['value'] = float(val) if pd.notna(val) and not np.isinf(val) else None
            
            if result.isna().any():
                test_result['has_nans'] = True
            
            valid_values = result.replace([np.inf, -np.inf], np.nan).dropna()
            if len(valid_values) > 0:
                test_result['value_range']['value'] = {
                    'min': float(valid_values.min()),
                    'max': float(valid_values.max()),
                    'mean': float(valid_values.mean())
                }
        
        # Test incremental update
        try:
            new_candle = test_df.iloc[0].to_dict()
            update_result = indicator.update(new_candle)
            test_result['incremental_update'] = 'success'
        except Exception as e:
            test_result['incremental_update'] = f'failed: {str(e)[:50]}'
        
        test_result['status'] = 'passed'
        test_results['passed'] += 1
        print(f"‚úÖ PASS ({calc_time:.1f}ms)")
        
    except Exception as e:
        test_result['status'] = 'failed'
        test_result['error'] = str(e)[:200]
        test_results['failed'] += 1
        print(f"‚ùå FAIL - {str(e)[:50]}")
    
    test_results['total_tested'] += 1
    test_results['detailed_results'].append(test_result)

# Generate summary
print(f"\n{'='*100}")
print(f"TEST SUMMARY")
print(f"{'='*100}\n")

print(f"Total Indicators Tested: {test_results['total_tested']}")
print(f"‚úÖ Passed: {test_results['passed']} ({test_results['passed']/test_results['total_tested']*100:.1f}%)")
print(f"‚ùå Failed: {test_results['failed']} ({test_results['failed']/test_results['total_tested']*100:.1f}%)")

# Group by status
passed_indicators = [r for r in test_results['detailed_results'] if r['status'] == 'passed']
failed_indicators = [r for r in test_results['detailed_results'] if r['status'] == 'failed']

# Calculate averages for passed indicators
if passed_indicators:
    avg_calc_time = sum(r['calculation_time_ms'] for r in passed_indicators) / len(passed_indicators)
    print(f"\nüìä Performance Metrics:")
    print(f"   Average calculation time: {avg_calc_time:.2f}ms")
    print(f"   Fastest: {min(r['calculation_time_ms'] for r in passed_indicators):.2f}ms ({min(passed_indicators, key=lambda x: x['calculation_time_ms'])['name']})")
    print(f"   Slowest: {max(r['calculation_time_ms'] for r in passed_indicators):.2f}ms ({max(passed_indicators, key=lambda x: x['calculation_time_ms'])['name']})")

# Show failed indicators
if failed_indicators:
    print(f"\n‚ùå Failed Indicators ({len(failed_indicators)}):")
    for r in failed_indicators:
        print(f"   - {r['name']}: {r['error']}")

# Save detailed results
output_file = 'indicator_test_results.json'
with open(output_file, 'w') as f:
    json.dump(test_results, f, indent=2)

print(f"\n‚úÖ Detailed results saved to: {output_file}")

# Generate markdown report
report_file = 'INDICATOR_TEST_REPORT.md'
with open(report_file, 'w') as f:
    f.write("# Comprehensive Indicator Test Report\n\n")
    f.write(f"**Test Date:** October 1, 2024\n")
    f.write(f"**Total Indicators:** {test_results['total_tested']}\n")
    f.write(f"**Passed:** {test_results['passed']} ({test_results['passed']/test_results['total_tested']*100:.1f}%)\n")
    f.write(f"**Failed:** {test_results['failed']} ({test_results['failed']/test_results['total_tested']*100:.1f}%)\n\n")
    
    f.write("## Summary Statistics\n\n")
    if passed_indicators:
        f.write(f"- Average Calculation Time: {avg_calc_time:.2f}ms\n")
        f.write(f"- Fastest Indicator: {min(passed_indicators, key=lambda x: x['calculation_time_ms'])['name']} ({min(r['calculation_time_ms'] for r in passed_indicators):.2f}ms)\n")
        f.write(f"- Slowest Indicator: {max(passed_indicators, key=lambda x: x['calculation_time_ms'])['name']} ({max(r['calculation_time_ms'] for r in passed_indicators):.2f}ms)\n\n")
    
    f.write("## Passed Indicators\n\n")
    f.write("| # | Indicator | Calc Time (ms) | Output Columns | Incremental Update |\n")
    f.write("|---|-----------|----------------|----------------|--------------------|\n")
    for idx, r in enumerate(passed_indicators, 1):
        cols = ', '.join(r['output_columns'][:3])
        if len(r['output_columns']) > 3:
            cols += f" (+{len(r['output_columns'])-3} more)"
        update_status = r.get('incremental_update', 'N/A')
        update_icon = '‚úÖ' if update_status == 'success' else '‚ùå'
        f.write(f"| {idx} | `{r['name']}` | {r['calculation_time_ms']:.2f} | {cols} | {update_icon} |\n")
    
    if failed_indicators:
        f.write("\n## Failed Indicators\n\n")
        f.write("| # | Indicator | Error |\n")
        f.write("|---|-----------|-------|\n")
        for idx, r in enumerate(failed_indicators, 1):
            error_msg = r['error'][:100] + '...' if len(r['error']) > 100 else r['error']
            f.write(f"| {idx} | `{r['name']}` | {error_msg} |\n")
    
    f.write("\n## Detailed Test Results\n\n")
    for r in test_results['detailed_results']:
        f.write(f"### {r['name']}\n\n")
        f.write(f"- **Status:** {'‚úÖ PASSED' if r['status'] == 'passed' else '‚ùå FAILED'}\n")
        f.write(f"- **Class:** `{r['class']}`\n")
        f.write(f"- **Calculation Time:** {r['calculation_time_ms']:.2f}ms\n")
        if r['output_columns']:
            f.write(f"- **Output Columns:** {', '.join([f'`{c}`' for c in r['output_columns']])}\n")
        if r['sample_values']:
            f.write(f"- **Sample Values:** {json.dumps(r['sample_values'], indent=2)}\n")
        if r['error']:
            f.write(f"- **Error:** `{r['error']}`\n")
        f.write("\n")

print(f"‚úÖ Markdown report saved to: {report_file}")

print(f"\n{'='*100}\n")
